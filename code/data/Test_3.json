[{"Header": "Lecture 4:  Data Mining Data (III)", "Header_keywords": ["data mining data"], "Paragraph_keywords": ["min chi department", "computer science north carolina state university"], "slides": [1]}, {"Header": "Outline", "Header_keywords": [], "Paragraph_keywords": ["data quality"], "slides": [2]}, {"Header": "Data Preprocessing", "Header_keywords": ["data preprocessing"], "Paragraph_keywords": ["attribute transformation", "feature creation", "feature subset selection", "dimensionality reduction"], "slides": [3, 13, 18, 22]}, {"Header": "Iris  Data", "Header_keywords": ["iris data"], "Paragraph_keywords": ["length petal", "width class"], "slides": [4]}, {"Header": "Discretization", "Header_keywords": [], "Paragraph_keywords": ["petal width", "categorical values"], "slides": [5, 7, 8, 9]}, {"Header": "Discretization Without Using Class  Labels", "Header_keywords": ["class labels"], "Paragraph_keywords": ["data equal interval width equal frequency"], "slides": [6]}, {"Header": "", "Header_keywords": [], "Paragraph_keywords": ["unique tuple", "petal width", "petal length", "corresponding count value", "steep slope", "shallow slope", "simple matrix algebra", "eigenvector example", "column vector", "equal magnitude", "diagonal elements", "uare matrices", "uare matrix", "symmetric matrix", "real eigenvalues", "orthogonal eigenvectors", "diagonal matrix", "principal component", "original variables", "new variables", "original variance", "original pca data pca example data", "original data", "covariance matrix", "nondiagonal elements", "choosing components", "feature vector", "mean adjusted data", "normalised data", "transformed data", "single eigenvector", "eigenvector original pca data pca example data"], "slides": [10, 11, 26, 39, 42, 47, 52, 53, 54, 55, 56, 58, 61, 62, 63, 65, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76]}, {"Header": "Resulted a multidimensional array", "Header_keywords": ["multidimensional array"], "Paragraph_keywords": ["count attribute"], "slides": [12]}, {"Header": "Attribute Transformation", "Header_keywords": [], "Paragraph_keywords": ["entire set", "given attribute", "new set", "replacement values", "old value", "new values", "simple functions"], "slides": [14]}, {"Header": "AFribute TransformaRon in AcRon", "Header_keywords": [], "Paragraph_keywords": [], "slides": [15]}, {"Header": "ADribute Transforma8on in Ac8on", "Header_keywords": ["adribute transformaon"], "Paragraph_keywords": [], "slides": [16]}, {"Header": "Try di\ufb00erent ones", "Header_keywords": ["dierent ones"], "Paragraph_keywords": [], "slides": [17]}, {"Header": "Curse of Dimensionality", "Header_keywords": [], "Paragraph_keywords": ["compute difference", "max min distance"], "slides": [19]}, {"Header": "Feature Subset Selection", "Header_keywords": ["subset selection"], "Paragraph_keywords": ["redundant features", "purchase price", "sales tax", "irrelevant features", "data mining task", "students gpa", "bruteforce approach", "possible feature subsets", "data mining algorithm", "embedded approaches", "feature selection", "filter approaches", "black box"], "slides": [20, 21]}, {"Header": "Dimensionality Reduction", "Header_keywords": ["dimensionality reduction"], "Paragraph_keywords": ["data mining algorithms", "irrelevant features", "principal component analysis", "singular value decomposition", "supervised nonlinear techniques"], "slides": [23]}, {"Header": "2d Data", "Header_keywords": [], "Paragraph_keywords": [], "slides": [24, 59]}, {"Header": "Dimensionality ReducRon PCA", "Header_keywords": ["dimensionality reducron pca"], "Paragraph_keywords": ["covariance matrix", "new space"], "slides": [25, 28, 60, 67]}, {"Header": "Principal Components", "Header_keywords": ["principal components"], "Paragraph_keywords": ["principal vector", "principal vectors", "best axis", "minimum rms error principal vectors"], "slides": [27, 29, 64]}, {"Header": "PCA", "Header_keywords": [], "Paragraph_keywords": ["principal components analysis pca", "high dimensional data", "lower dimensional linear subspace original axes", "data points", "principal component", "second principal component"], "slides": [30]}, {"Header": "PCA on Faces  \u201c Eigenfaces \u201d", "Header_keywords": [], "Paragraph_keywords": ["average face", "principal component"], "slides": [31]}, {"Header": "PCA for RelighRng", "Header_keywords": [], "Paragraph_keywords": ["dierent illuminaron", "matusik mcmillan", "rst principal components"], "slides": [32, 33]}, {"Header": "PCA  Example", "Header_keywords": ["pca example"], "Paragraph_keywords": [], "slides": [34, 35, 36]}, {"Header": "10 variables 10 Eigenvalues", "Header_keywords": ["variables eigenvalues"], "Paragraph_keywords": [], "slides": [37, 40]}, {"Header": "Total Variance", "Header_keywords": ["total variance"], "Paragraph_keywords": [], "slides": [38]}, {"Header": "Eigenvalue Scree Plot", "Header_keywords": ["eigenvalue scree plot"], "Paragraph_keywords": [], "slides": [41]}, {"Header": "What are the Principle Components", "Header_keywords": ["principle components"], "Paragraph_keywords": [], "slides": [43, 44, 45, 46]}, {"Header": "Transformation Matrices", "Header_keywords": ["transformation matrices"], "Paragraph_keywords": ["scale vector", "square transformation matrix", "important observation", "given transformation matrix"], "slides": [48, 49]}, {"Header": "Eigenvalue Problem", "Header_keywords": ["eigenvalue problem"], "Paragraph_keywords": ["eigenvalue problem", "following form", "nonzero vector", "square matrix"], "slides": [50, 51]}, {"Header": "PCA, what is our A?", "Header_keywords": [], "Paragraph_keywords": ["covariance matrix"], "slides": [57]}, {"Header": "PCA feature selecRon and feature  extracRon", "Header_keywords": ["pca feature"], "Paragraph_keywords": ["thousands length vehicle type", "component scores"], "slides": [77]}]